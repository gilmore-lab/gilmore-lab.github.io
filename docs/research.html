<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>research – BBD Lab</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-79108a0fc1995748cbd19a5b0e3e3e7c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">BBD Lab</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./research.html" aria-current="page"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./parents.html"> 
<span class="menu-text">For Parents</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./who-we-are.html"> 
<span class="menu-text">Who we are</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-lab-mtgs" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Lab mtgs</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-lab-mtgs">    
        <li>
    <a class="dropdown-item" href="./lab-meetings.html">
 <span class="dropdown-text">Current semester</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./lab-meetings-archive.html">
 <span class="dropdown-text">Past semesters</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./resources.html"> 
<span class="menu-text">Resources</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./site-info.html"> 
<span class="menu-text">Site info</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/gilmore-lab/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text">Github</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta">

    
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">February 5, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<section id="research" class="level1">
<h1>Research</h1>
<p>This page describes some of the research the lab group is or has engaged in.</p>
<section id="play-learning-across-a-year-play" class="level2">
<h2 class="anchored" data-anchor-id="play-learning-across-a-year-play">Play &amp; Learning Across a Year (PLAY)</h2>
<p><img src="https://www.play-project.org/img/PLAY-logo.png" width="480px/"></p>
<p>The Play &amp; Learning Across a Year (PLAY) project serves as a model system for doing development science from a “big” data approach. Natural free play represents the foundation of infant learning, but we know little about how infants play, how play unfolds in real time and across development, and how individual and group differences promote infant learning and development through play. To answer these questions, the PLAY project will collect, code, and share 900 hours of video collected in the homes of children at 12, 18, and 24 months of age drawn from 30 sites across North America.</p>
<p>The aim of the project is to develop a new approach to developmental science that enables (1) “big” data science for researchers who would not otherwise have access; (2) a communal, low-cost means of collecting and coding data that retains the autonomy of individual labs; and (3) a plan for leveraging diverse expertise to address a common goal.</p>
<p><strong>Publications</strong></p>
<p>Soska, K.C., Xu, M., Gonzalez, S.L., Herzberg, O., Tamis-LeMonda, C.S., Gilmore, R.O., &amp; Adolph, K.E. (2021). (Hyper)active data curation: A video case study from behavioral science. <em>Journal of eScience Librarianship</em>, <em>10</em>(3), e1208. <a href="https://doi.org/10.7191/jeslib.2021.1208" class="uri">https://doi.org/10.7191/jeslib.2021.1208</a>.</p>
<p><strong>Presentations</strong></p>
<p>Gilmore, R.O., Jayaraman, S., Hassan, S., &amp; Lingeman, J. (2020, July). Tools for reproducible developmental science. Poster to be presented at the 2020 International Congress on Infant Studies, Glasgow, Scotland.</p>
<p>Adolph, K.E., Tamis-LeMonda, C., &amp; Gilmore, R.O. (2016, December 16). Video-based communal data collection &amp; coding: Advancing the science of infant learning &amp; development. A workshop held at the Eunice Kennedy Shriver National Institute of Child Health and Human Development. <a href="https://videocast.nih.gov/summary.asp?Live=20371&amp;bhcp=1">Video Cast</a>. <a href="https://nyu.databrary.org/volume/254">Materials &amp; data</a>.</p>
<p><strong>Materials</strong></p>
<p>Gilmore, R.O. (2017). PLAY-behaviorome. Github code repository. Retrieved December 19, 2017 from <a href="https://github.com/PLAY-behaviorome/" class="uri">https://github.com/PLAY-behaviorome/</a>.</p>
<p>Adolph, K., Tamis-LeMonda, C. &amp; Gilmore, R.O. (2017). PLAY Pilot Data Collections. Databrary. Retrieved December 19, 2017 from <a href="https://nyu.databrary.org/volume/444" class="uri">https://nyu.databrary.org/volume/444</a>.</p>
<p>Adolph, K., Tamis-LeMonda, C. &amp; Gilmore, R.O. (2016). PLAY Project: Materials. Databrary. Retrieved December 19, 2017 from <a href="https://nyu.databrary.org/volume/254" class="uri">https://nyu.databrary.org/volume/254</a>.</p>
<p>Adolph, K., Tamis-LeMonda, C. &amp; Gilmore, R.O. (2016). PLAY Project: Webinar discussions on protocol and coding. Databrary. Retrieved December 19, 2017 from <a href="https://nyu.databrary.org/volume/232" class="uri">https://nyu.databrary.org/volume/232</a>.</p>
<p><strong>Role</strong></p>
<ul>
<li>Serve as Co-PI and head of data science.</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Karen Adolph, New York University, Co-Principal Investigator.</li>
<li>Catherine Tamis-LeMonda, New York University, Co-Principal Investigator.</li>
<li>Kasey Soska, New York University, Scientific Director.</li>
</ul>
<p><strong>Support</strong></p>
<p>PLAY is supported by grants from the Office of the Director, National Institutes of Health, (OD), Eunice Kennedy Shriver National Institute for Child Health and Human Development (NICHD), the National Institute of Mental Health (NIMH), and the National Institute on Drug Abuse (NIDA) under R01HD094830-01, the LEGO Foundation, and the Alfred P. Sloan Foundation.</p>
</section>
<section id="open-science" class="level2">
<h2 class="anchored" data-anchor-id="open-science">Open Science</h2>
<p>The social, behavioral, and neural sciences face more difficult scientific challenges than any other field has faced before. Open, transparent, and reproducible scientific practices are essential for accelerating discovery in these fields. My colleagues and I are developing policies for the ethical and secure sharing of personal data and technologies to allow the analysis and sharing of these data for scientific and educational purposes.</p>
<p><strong>Publications</strong></p>
<p>Soska, K.C., Xu, M., Gonzalez, S.L., Herzberg, O., Tamis-LeMonda, C.S., Gilmore, R.O., &amp; Adolph, K.E. (2021). (Hyper)active data curation: A video case study from behavioral science. <em>Journal of eScience Librarianship</em>, <em>10</em>(3), e1208. <a href="https://doi.org/10.7191/jeslib.2021.1208" class="uri">https://doi.org/10.7191/jeslib.2021.1208</a>.</p>
<p>Gilmore, R.O., &amp; Qian, Y. (2021). An open developmental science will be more rigorous, robust, and impactful. <em>Infant and Child Development</em>. <a href="https://doi.org/10.1002/icd.2254" class="uri">https://doi.org/10.1002/icd.2254</a>.</p>
<p>Gilmore, R.O., Xu, M., &amp; Adolph, K.E. (2021). Data sharing. In Panecker, S. &amp; Stanley, B. (Eds.), <em>Handbook of Research Ethics in Psychological Science</em>, APA Press, Washington, DC. <a href="pdfs/gilmore-xu-adolph-2021.pdf">PDF</a>.</p>
<p>Gilmore, R.O., Cole, P.M., van Aken, M.A.G., Verma S., &amp; Worthman, C.M. (2020). Advancing scientific integrity, transparency, and openness in child development research: Challenges and possible solutions. <em>Child Development Perspectives</em>, <em>14</em>(1), 9-14. <a href="http://dx.doi.org/10.1111/cdep.12360" class="uri">http://dx.doi.org/10.1111/cdep.12360</a></p>
<p>Ossmy O., Gilmore R.O., Adolph K.E. (2020) AutoViDev: A Computer-Vision Framework to Enhance and Accelerate Research in Human Development. In: Arai K., Kapoor S. (eds) <em>Advances in Computer Vision. CVC 2019. Advances in Intelligent Systems and Computing</em>, vol 944. Springer, Cham. <a href="https://doi.org/10.1007/978-3-030-17798-0_14">doi: 10.1007/978-3-030-17798-0_14</a></p>
<p>Gilmore, R. O., Kennedy, J. L., &amp; Adolph, K. E. (2018). Practical solutions for sharing data and materials From psychological research. <em>Advances in Methods and Practices in Psychological Science</em>, SAGE Publications Inc.&nbsp;Retrieved from https://doi.org/10.1177/2515245917746500. <a href="https://osf.io/rw7f3/">OSF preprint</a>.</p>
<p>Gilmore, R.O., Diaz, M.T., Wyble, B.A., &amp; Yarkoni, T. (2017). Progress toward openness, transparency, and reproducibility in cognitive neuroscience. <em>Annals of the New York Academy of Sciences</em>, 1396, 5–18. <a href="http://doi.org/10.1111/nyas.13325">doi: 10.1111/nyas.13325</a>.</p>
<p>Gilmore, R.O. (2016). From big data to deep insight in developmental science. <em>Wiley Interdisciplinary Reviews Cognitive Science</em>. <a href="http://doi.org/10.1001/wcs.1379">DOI: 10.1002/wcs.1379</a>.</p>
<p><strong>Presentations</strong></p>
<p>Gilmore, R.O., Wham, B., &amp; Zinoble, R. (2021, October). Getting ahead of the curve: Responding to emerging data management plan requirements. Presentation as part of the Open Data &amp; Developmental Science (ODDS) Initiative. <a href="https://psu-psychology.github.io/2021-10-01-odds-data-mgmt/">slides</a>.</p>
<p>Gilmore, R.O. (2021, April). Invited panel presentation to “Changing the Culture of Data Management and Sharing: A Workshop”, National Academies of Science, Engineering, and Medicine.</p>
<p>Gilmore, R.O. (2021, January). “The Human Behaviorome Project”, Invited presentation to the Networking and Information Technology Research and Development (NITRD) workshop on the Future of Federally Supported Data Repositories.</p>
<p>Gilmore, R.O. (2020, September). Invited presentation at FLUX Preconference Workshop on Infant Neuroimaging.</p>
<p>Gilmore, R.O. (2020, May 23). Invited symposium presentation on <em>Open Science for Different Methodological Approaches in Psychology</em>, 2020 meeting of the Association for Psychological Science, Chicago, IL. Cancelled.</p>
<p>Gilmore, R.O. (2020, April 15). The open science revolution: A report from the front lines. Talk given in the Penn State Data Studies Group. <a href="https://gilmore-lab.github.io/2020-04-15-data-studies-group/">slides</a>.</p>
<p>Gilmore, R.O. (2020, February 21). Research computing in child development research. Invited talk to the Penn State Child Study Center. <a href="https://psu-psychology.github.io/open-data-and-developmental-science-ODDS/2020-02-21-csc.html">slides</a>.</p>
<p>Gilmore, R.O. (2019, June 3). Making cognitive science even better. Talk given at the James S. McDonnell Foundation retreat. <a href="https://gilmore-lab.github.io/2019-06-03-McDonnell-Fdn/">slides</a>.</p>
<p>Gilmore, R.O. (2019, March 28). The whole elephant. Talk given at the Penn State Center for Neural Engineering (CNE) colloquium series. <a href="https://gilmore-lab.github.io/2019-03-28-cne/">slides</a>.</p>
<p>Gilmore, R.O., Gennetian, L., Kalish, C., Tamis-LeMonda, C.T., &amp; Worthman, C. (2019, March). What SRCD is doing about open science: A conversation hour. Presentation at the 2019 Society for Research in Child Development (SRCD) meeting, Baltimore, MD. <a href="https://gilmore-lab.github.io/2019-03-22-SRCD-conversation/">slides</a>.</p>
<p>Gilmore, R.O. (2019, January). An open science of human health &amp; behavior. Invited talk given to the College of Health &amp; Human Development. <a href="https://gilmore-lab.github.io/2019-01-15-open-science-psu-hhd/">HTML slides</a>.</p>
<p>Gilmore, R.O. (2018, October). The Open Data and Developmental Science (ODDS) Initiative. Invited talk at the Penn State Child Study Center <a href="http://csc.la.psu.edu/">(CSC)</a>. <a href="https://gilmore-lab.github.io/2018-10-12-ODDS/">HTML slides</a>.</p>
<p>Gilmore, R.O. (2018, October). The promise of open developmental science. Invited talk at the <a href="https://convention2.allacademic.com/one/srcd/devsec18/index.php?cmd=Prepare+Online+Program&amp;program_focus=main&amp;PHPSESSID=ghs9iuu3994o7nv5bd03p5d1g3">SRCD Conference on the Use of Secondary and Open Source Data in Developmental Science</a>. Phoenix, AZ. <a href="http://gilmore-lab.github.io/DEVSEC-2018/promise-of-open-dev-sci/">HTML slides</a></p>
<p>Gilmore, R.O. (2018, September). Big data behavioral science: From micro- to macro-scale. Annual Conference of the Federal Statistical Research Data Centers. University Park, PA. <a href="http://gilmore-lab.github.io/2018-09-07-fsrdc">HTML slides</a></p>
<p>Gilmore, R.O. (2018, May). Open, says me: Practical solutions for sharing data and materials. Invited talk at the Association for Psychological Science 2018 meeting. San Francisco, CA. <a href="http://gilmore-lab.github.io/2018-05-26-aps-opensaysme">HTML slides</a></p>
<p>Gilmore, R.O. (2017, September 29). Data sharing, research ethics, &amp; scientific reproducibility. Talk at the Scholarship and Research Integrity (SARI) workshop series, Penn State. <a href="http://gilmore-lab.github.io/psu-sari-2017-09-28/">HTML slides</a>.</p>
<p>Gilmore, R.O. (2017, September 7). Reproducibility in computationally intensive behavioral research. Talk at the ACI-ICS seminar series, Penn State. <a href="http://gilmore-lab.github.io/aci-ics-2017-09-07/">HTML slides</a>.</p>
<p><strong>Software</strong></p>
<p><em>databraryapi</em>, an R package for Databrary. <a href="https://github.com/PLAY-behaviorome/databraryapi" class="uri">https://github.com/PLAY-behaviorome/databraryapi</a>.</p>
<p><em>databrarypy</em>, a Python package for Databrary. <a href="https://github.com/PLAY-behaviorome/databrarypy" class="uri">https://github.com/PLAY-behaviorome/databrarypy</a>.</p>
<p><strong>Roles</strong></p>
<ul>
<li>Chaired Society for Research in Child Development (SRCD) Task Force on Scientific Integrity and Openness (2018-2019) that drafted a <a href="https://www.srcd.org/policy-scientific-integrity-transparency-and-openness">Policy on Scientific Integrity, Transparency, and Openness</a> and <a href="https://www.srcd.org/research/journals/child-development/author-guidelines-scientific-integrity-and-openness-child-development">author guidelines</a>.</li>
<li>Lead the <a href="https://github.com/psu-psychology/open-data-and-developmental-science-ODDS">Open Data &amp; Developmental Science (ODDS) initiative</a> for the Penn State <a href="https://csc.la.psu.edu/">Child Study Center</a>.</li>
<li>Co-lead R Bootcamp workshop for Penn State graduate students, postdocs, and faculty. <a href="https://psu-psychology.github.io/r-bootcamp-2019/">2019 course site</a>, <a href="https://psu-psychology.github.io/r-bootcamp-2018/">2018 course site</a>.</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Karen Adolph, New York University, Co-Principal Investigator and Project Director.</li>
<li>Ori Ossmy, New York University</li>
<li>Jeff Spies, 221b.io</li>
</ul>
</section>
<section id="databrary" class="level2">
<h2 class="anchored" data-anchor-id="databrary">Databrary</h2>
<p><img src="http://databrary.org/theme/img/logo/databrary.png"></p>
<p>The <a href="http://databrary.org">Databrary Project</a> aims to increase scientific transparency and accelerate discovery in developmental science by building infrastructure for researchers to share video data and related meta-data. The project has five specific aims:</p>
<ul>
<li>Create a web-based <a href="http://databrary.org">data library</a> for sharing and preserving video data and associated meta-data.</li>
<li>Create participant and contributor/user standards that enable open sharing of video data while limiting access to authorized users to ensure participant confidentiality.</li>
<li>Expand the free, open source video coding software, <a href="http://datavyu.org">Datavyu</a> to enable coding, exploring, and analyzing video data.</li>
<li>Build a data management system to support data sharing within labs, among collaborators, and in the Databrary repository.</li>
<li>Transform the culture of developmental science by building a community of researchers committed to open video data sharing.</li>
</ul>
<p>Databrary is an <a href="http://github.com/databrary">open-source</a> software project. Penn State is one of the major “nodes”, with a large number of authorized users.</p>
<p><strong>Publications</strong></p>
<p>Gilmore, R.O., Xu, M., &amp; Adolph, K.E. (2021). Data sharing. In Panecker, S. &amp; Stanley, B. (Eds.), <em>Handbook of Research Ethics in Psychological Science</em>, APA Press, Washington, DC.</p>
<p>Gilmore, R. O. &amp; Adolph, K. E. (2019). Open sharing of research video: Breaking down the boundaries of the research team. In , K. L. Hall, A. L. Vogel, &amp; R. T. Croyle (Eds.), <em>Strategies for team science success: Handbook of evidence-based principles for cross-disciplinary science and practical lessons learned from health researchers</em>. Cham: Springer, pp.&nbsp;575-583.</p>
<p>Gilmore, R. O., Kennedy, J. L., &amp; Adolph, K. E. (2018). Practical solutions for sharing data and materials From psychological research. <em>Advances in Methods and Practices in Psychological Science</em>, SAGE Publications Inc.&nbsp;Retrieved from https://doi.org/10.1177/2515245917746500. <a href="https://osf.io/rw7f3/">OSF preprint</a>.</p>
<p>Adolph, K.E., Gilmore, R.O., &amp; Kennedy, J.L. (2017, October). Video data and documentation will improve psychological science. <em>Psychological Science Agenda</em>. <a href="http://www.apa.org/science/about/psa/2017/10/video-data.aspx" class="uri">http://www.apa.org/science/about/psa/2017/10/video-data.aspx</a></p>
<p>Gilmore, R. O., Kennedy, J. L., &amp; Adolph, K. E. (2017, September 7). Practical solutions for the ethical sharing of identifiable research data. Retrieved from <a href="http://psyarxiv.com/kew8u" class="uri">http://psyarxiv.com/kew8u</a>.</p>
<p>Gilmore, R.O. &amp; Adolph, K.E. (2017). Video can make behavioural science more reproducible. <em>Nature Human Behaviour</em>. <a href="http://doi.org/10.1038/s41562-017-0128">doi:10.1038/s41562-017-0128</a>.</p>
<p>Gilmore, R.O., &amp; Adolph, K.E. (2017, February 6). Video can make science more open, transparent, robust, and reproducible. Retrieved from <a href="http://osf.io/3kvp7" class="uri">http://osf.io/3kvp7</a>.</p>
<p>Gilmore, R.O., Adolph, K.E., Millman, D.S. (2016). Curating identifiable data for sharing: The Databrary project. In Proceedings of the 2016 New York Scientific Data Summit. <a href="https://github.com/databrary/presentations/blob/master/nysds-2016/gilmore-adolph-millman-nysds-2016.pdf">PDF of paper</a>.</p>
<p>Gilmore, R.O., Adolph, K.E., Millman, D.S., &amp; Gordon, A. (2016). Transforming education research through open video data sharing. <em>Advances in Engineering Education</em>, <em>5</em>(2). <a href="http://advances.asee.org/publication/transforming-education-research-through-open-video-data-sharing/">HTML</a>.</p>
<p>Gordon, A., Millman, D.S., Steiger, L., Adolph, K.E., &amp; Gilmore, R.O. (2015). Researcher-library collaborations: Data repositories as a service for researchers. <em>Journal of Librarianship and Scholarly Communication</em>. <a href="http://dx.doi.org/10.7710/2162-3309.1238">doi:10.7710/2162-3309.1238</a>.</p>
<p>Adolph, K.E., Gilmore, R.O., Freeman, C., Sanderson, P., &amp; Millman, D. (2012). Toward Open Behavioral Science, <em>Psychological Inquiry: An International Journal for the Advancement of Psychological Theory</em>, <em>23</em>(3), 244-247. <a href="http://dx.doi.org/10.1080/1047840X.2012.705133">doi:10.1080/1047840X.2012.705133</a>.</p>
<p><strong>Presentations</strong></p>
<p>Gilmore, R.O. (2019, June 6). Video as data and documentation. Workshop at a Symposium Honoring Brian MacWhinney, Carnegie Mellon University, Pittsburgh, PA. <a href="https://gilmore-lab.github.io/2019-06-MacWhinney-Symposium/databrary-workshop/">slides</a>.</p>
<p>Gilmore, R.O. (2018, October). Sharing video data. Data blitz talk at the <a href="https://convention2.allacademic.com/one/srcd/devsec18/index.php?cmd=Prepare+Online+Program&amp;program_focus=main&amp;PHPSESSID=ghs9iuu3994o7nv5bd03p5d1g3">SRCD Conference on the Use of Secondary and Open Source Data in Developmental Science</a>. Phoenix, AZ. <a href="https://gilmore-lab.github.io/DEVSEC-2018/data_blitz/">HTML slides</a></p>
<p>Gilmore, R.O., Adolph, K.E., &amp; Seisler, A.S. (2018, October). Sharing nicely with others: Moving developmental scientists toward open data sharing. Poster presented at the <a href="https://convention2.allacademic.com/one/srcd/devsec18/index.php?cmd=Prepare+Online+Program&amp;program_focus=main&amp;PHPSESSID=ghs9iuu3994o7nv5bd03p5d1g3">SRCD Conference on the Use of Secondary and Open Source Data in Developmental Science</a>. Phoenix, AZ. <a href="https://github.com/gilmore-lab/DEVSEC-2018/blob/master/poster/gilmore-adolph-seisler-devsec-2018-poster.pdf">PDF</a>.</p>
<p>Gilmore, R.O. (2018, April 25). Video as data and documentation. Talk in the Department of Communication Arts &amp; Sciences, Penn State. <a href="http://gilmore-lab.github.io/2018-04-25-introducing-databrary/">HTML slides</a></p>
<p>Gilmore, R.O. (2018, February 14). The future of quantitative developmental science. Talk given at the Quantitative Developmental Methodology brown bag series. <a href="http://gilmore-lab.github.io/2018-02-14-quant-dev/">HTML slides</a>.</p>
<p>Gilmore, R.O. (2018, January 31). Introducing Databrary. Talk given at the Penn State <a href="https://sites.psu.edu/librarynews/2017/12/11/software-in-the-humanities-and-social-sciences-workshops/">Software in the Humanities &amp; Social Sciences</a> workshop. <a href="https://gilmore-lab.github.io/2018-01-31-software-in-humanities/">HTML slides</a>.</p>
<p>Gilmore, R.O. (2018, January 26). The Video DatAbservatory: A platform for behavioral discovery. Talk at the Pathways to Competence initiative meeting of the Penn State Child Study Center. <a href="https://gilmore-lab.github.io/2018-01-26-p2c/">HTML slides</a>.</p>
<p>Gilmore, R.O. (2017, August 1). Video can improve the reproducibility of psychological science. Lightning talk at the Society for Improving Psychological Science meeting, Charlottesville, VA. <a href="https://gilmore-lab.github.io/sips-2017-video-reproducibility">HTML slides</a></p>
<p>Gilmore, R.O. (2017, July 31). Beyond physics envy: Toward a databservatory for human behavior. Lightning talk at the Society for Improving Psychological Science meeting, Charlottesville, VA. <a href="http://gilmore-lab.github.io/sips-2017-databservatory">HTML slides</a>.</p>
<p>Gilmore, R.O. &amp; Nilsonne, G. (2017, July 30). IRBs and the ethical sharing of research data. Talk given at the Society for Improving Psychological Science meeting, Charlottesville, VA. <a href="http://gilmore-lab.github.io/sips-2017-07-30/">HTML slides</a>. <a href="http://osf.io/9d5hr">OSF project</a>.</p>
<p>Gilmore, R.O. (2017, July 26). Yes, we can. Invited panelist at the AERA Workshop on Data Sharing and Research Transparency at the Article Publishing Stage. <a href="http://gilmore-lab.github.io/aera-workshop-2017-07-26/">HTML slides</a>.</p>
<p>Gilmore, R.O. (2017, July 10). The reproducibility crisis in psychology &amp; neuroscience. Talk given at the Penn State Data Reproducibility Bootcamp. <a href="http://gilmore-lab.github.io/psu-data-repro-bootcamp-2017-07-10/">HTML slides</a>.</p>
<p>Adolph, K.E., Binion, G. Gilmore, R.O., Oakes, L., &amp; Vazire, S. (2017, April 6). Openness, replication, &amp; data reuse in developmental science – unique challenges, existing resources, &amp; what is still needed. Invited roundtable at the Society for Research in Child Development meeting, Austin, TX.</p>
<p>Gilmore, R.O. (2017, February 22). A Databservatory for human behavior. Talk given at the Cognitive Area Brown Bag. <a href="https://gilmore-lab.github.io/cog-bbag-talk-2017-02-22">HTML slides</a>.</p>
<p>Gilmore, R.O. (2017, January 31). An -ome of our own: Toward a more reproducible, robust, and insightful science of human behavior. Talk given to the Social Data Analytics (SoDA) 501 students. Penn State University. <a href="http://gilmore-lab.github.io/soda-2017-01-31">HTML slides</a></p>
<p>Gilmore, R.O. (2016, October). The future of big data in developmental science. Talk given at a meeting of the Penn State Child Study Center (CSC) faculty. <a href="http://rawgit.com/gilmore-lab/psu-child-study-ctr-talk-2016-10-28/master/gilmore-csc-talk.html">HTML slides</a>.</p>
<p>Gilmore, R.O. (2016, September). Donald Rumsfeld and the promise of a ‘big data’ science of human behavior. Talk given at a meeting of the Stochastic Modeling and Computational Statistics (SMACS) group, Department of Statistics. <a href="https://rawgit.com/gilmore-lab/psu-stats-smacs-2016-talk/master/gilmore-smacs-2016-09-02.html">HTML slides</a>.</p>
<p>Gilmore, R.O., Adolph, K.E., &amp; Millman, D.S. (2016, August). Curating identifiable data for sharing: The Databrary project. In Proceedings of the 2016 New York Scientific Data Summit. <a href="https://rawgit.com/databrary/presentations/master/nysds-2016/gilmore-nysds-2016.html">HTML slides</a>. <a href="https://github.com/databrary/presentations/blob/master/nysds-2016/gilmore-adolph-millman-nysds-2016.pdf">PDF of paper</a>.</p>
<p>Gilmore, R.O., Adolph, K.E., &amp; Millman, D. (2016, May). Video doesn’t lie: Reproducible workflows with Databrary. Talk given at the NYU Data Science Center <a href="https://reproduciblescience.org/nyu/events/reproducibility-symposium-2016/schedule/">Symposium on Reproducibility</a>. <a href="https://rawgit.com/databrary/presentations/master/nyu-data-science-reproducibility-16/be-bold.html#1">HTML slides</a></p>
<p>Gilmore, R.O., Adolph, K.E., Millman, D.S., Steiger, L., &amp; Simon, D.A. (2015, May). Sharing displays and data from vision science research with Databrary. Poster presented at the Vision Sciences Society meeting, St.&nbsp;Pete Beach, FL. <a href="../pdfs/gilmore-etal-vss-2015.pdf">PDF</a>.</p>
<p>Simon, D.A., Gordon, A.S., Steiger, L., &amp; Gilmore, R.O. (2015, June). Databrary: Enabling sharing and reuse of research video. Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries, Knoxville, TN. <a href="https://doi.org/10.1145/2756406.2756951">doi:10.1145/2756406.2756951</a></p>
<p><strong>Role</strong></p>
<ul>
<li>Serve as co-founder and co-director.</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Karen Adolph, New York University, Co-Principal Investigator and Project Director.</li>
</ul>
<p><strong>Support</strong></p>
<p>This project is supported by the U.S. National Science Foundation (NSF) Grant No.&nbsp;<a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1238599">BCS-1238599</a>, the Eunice Kennedy Shriver National Institute of Child Health and Human Development under Cooperative Agreement <a href="http://projectreporter.nih.gov/project_info_description.cfm?aid=8531595&amp;icde=15908155&amp;ddparam=&amp;ddvalue=&amp;ddsub=&amp;cr=1&amp;csb=default&amp;cs=ASC">1-U01-HD-076595-01</a>, the Society for Research in Child Development, the LEGO Foundation, the Alfred P. Sloan Foundation, and the James S. McDonnell Foundation.</p>
</section>
<section id="sex-differences-in-visual-perception" class="level2">
<h2 class="anchored" data-anchor-id="sex-differences-in-visual-perception">Sex Differences in Visual Perception</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-22269-y/MediaObjects/41598_2022_22269_Fig2_HTML.png?as=webp" class="img-fluid figure-img"></p>
<figcaption><span class="citation" data-cites="Qian2022-yp">Qian, Berenbaum, &amp; Gilmore (<a href="#ref-Qian2022-yp" role="doc-biblioref">2022</a>)</span> Figure 2</figcaption>
</figure>
</div>
<p>Previous research has shown that males and females differ on a number of psychological dimensions, including some basic aspects of visual processing. We are exploring the extent to which low-level visual dimensions of visual processing correlate with one another and with higher order characteristics like spatial cognition and hobby choices.</p>
<p><strong>Role</strong></p>
<ul>
<li>Co-Investigator</li>
</ul>
<p><strong>Materials</strong></p>
<ul>
<li><a href="https://github.com/gilmore-lab/sex-differences-in-motion-perception">GitHub project</a></li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Sheri Berenbaum, Ph.D., Penn State</li>
<li>Yiming Qian, Ph.D., UC Davis</li>
</ul>
<p><strong>Publications</strong></p>
<p>Qian, Y., Berenbaum, S. A., &amp; Gilmore, R. O. (2022). Vision contributes to sex differences in spatial cognition and activity interests. Scientific Reports, 12(1), 17623. <a href="https://doi.org/10.1038/s41598-022-22269-y" class="uri">https://doi.org/10.1038/s41598-022-22269-y</a></p>
</section>
<section id="behavioral-analysis-through-video-behav.ai" class="level2">
<h2 class="anchored" data-anchor-id="behavioral-analysis-through-video-behav.ai">Behavioral Analysis through Video (BehAV.ai)</h2>
<p>AI and computer vision tools are increasingly useful in basic and applied behavioral science. This project aims to provide a set of tools based on open source algorithms and models that empowers non-specialists to exploit advances in these areas.</p>
<p><strong>Materials</strong></p>
<ul>
<li><a href="https://github.com/behAV/">GitHub project</a></li>
</ul>
<p><strong>Role</strong></p>
<ul>
<li>Principal Investigator</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Dan Albohn, Penn State Department of Psychology</li>
<li>Kory Blose, Penn State Applied Research Lab</li>
<li>Stephen Fast, Penn State Applied Research Lab</li>
<li>Ori Ossmy, New York University</li>
<li>Drew Polasky, Penn State Institute for Cyberscience</li>
</ul>
</section>
<section id="developmental-dynamics-of-optic-flow-processing" class="level2">
<h2 class="anchored" data-anchor-id="developmental-dynamics-of-optic-flow-processing">Developmental Dynamics of Optic Flow Processing</h2>
<p><img src="images/fesi-2014.jpg" class="img-fluid" alt="Brain activity"> <img src="images/optic-flow.jpg" class="img-fluid" alt="Optic flow"></p>
<p>Visual motion provides humans and animals with information about their own movement through 3D space and about the structure of the environment – the objects, surfaces, and other animals that it may contain. How the human brain processes complex motion information poses an as-yet unanswered question. This project focuses on characterizing how sensitivity to visual motion emerges in the developing human brain: how brain (EEG) responses to patterns of ego- and object motion emerge, how they develop from infancy through childhood into adulthood, how specific changes in cortical circuitry might account for the observed patterns, and how behavioral sensitivity to motion corresponds to neural activation. The studies compare brain responses and behavioral discrimination patterns in infants, children, and adults to the same types of ego- and object motion. The studies also involve an effort to measure or simulate the statistics of optic flow experienced by infant, child, and adult observers in complex, natural environments using computer vision methods.</p>
<p><strong>Publications</strong></p>
<p>Qian, Y., Seisler, A.R., &amp; Gilmore, R.O. (in press). Children’s sensitivity to optic flow-like visual motion differs from adults. <em>Developmental Psychology</em>. GitHub repository at <a href="https://github.com/gilmore-lab/sex-differences-in-motion-perception">https://github.com/gilmore-lab/sex-differences-in-motion-perception</a></p>
<p>Gilmore, R.O., Thomas, A.L., &amp; Fesi, J.D (2016). Children’s brain responses to optic flow vary by pattern type and motion speed. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0157911">doi: 10.1371/journal.pone.0157911</a>. Materials on Databrary at <a href="http://doi.org/10.17910/B7QG6W">http://doi.org/10.17910/B7QG6W</a>.</p>
<p>Gilmore, R.O., Raudies, F., &amp; Jayaraman, S. (2015). What Accounts for Developmental Shifts in Optic Flow Sensitivity? <em>Proceedings of the IEEE International Conference on Development and Learning and Epigenetic Robotics</em>. <a href="http:/doi.org/10.1109/DEVLRN.2015.7345450">doi:10.1109/DEVLRN.2015.7345450</a>. Materials on Databrary at <a href="http://dx.doi.org/10.17910/B7988V">doi:10.17910/B7988V</a>.</p>
<p>Fesi, J.F., Thomas, A.L., &amp; Gilmore, R.O. (2014). Cortical responses to optic flow and motion contrast across patterns and speeds. <em>Vision Research</em>, 100, 56–71. <a href="http://dx.doi.org/10.1016/j.visres.2014.04.004">doi:10.1016/j.visres.2014.04.004</a>. <a href="https://databrary.org/volume/49">Materials on Databrary</a>.</p>
<p>Raudies, F. &amp; Gilmore, R.O. (2014). Visual motion priors differ for infants and mothers. <em>Neural Computation</em>, <em>26</em>(11), 2652-2668. <a href="http://dx.doi.org/10.1162/NECO_a_00645">doi:10.1162/NECO_a_00645</a>.</p>
<p>Raudies, F., Gilmore, R.O., Kretch, K.S., Franchak, J.M, &amp; Adolph, K.E. (2012). Understanding the development of motion processing by characterizing optic flow experienced by infants and their mothers. <em>Proceedings of the IEEE International Conference on Development and Learning</em>. <a href="http://dx.doi.org/10.1109/DevLrn.2012.6400584">doi:10.1109/DevLrn.2012.6400584</a>.</p>
<p><strong>Presentations</strong></p>
<p>Gilmore, R.O. (2018, February 24). The development of perception for action. Data blitz talk at the Developmental Area graduate recruiting weekend. <a href="http://gilmore-lab-github.io/2018-02-24-devel-recruit">HTML slides</a>.</p>
<p>Gilmore, R.O., Seisler, A., Shade, M., O’Neill, M. (2017, April). School-age children perceive fast radial optic flow in noise more accurately than slow linear flow. Poster presentation at the Society for Research in Child Development, Austin, TX. <a href="https://github.com/gilmore-lab/child-motion-psychophysics/blob/master/pubs/srcd-17-poster/gilmore-seisler-shade-oneill-srcd-2017.pdf">PDF</a>. <a href="http://nyu.databrary.org/volume/218">Databrary</a>. <a href="https://github.com/gilmore-lab/moco-3-pattern-psychophysics/tree/master/child-motion-psychophysics">GitHub</a>.</p>
<p>Gilmore, R.O. (2017, February). Go with the flow: The development of behavioral sensitivity and brain responses to optic flow. Talk at Temple University. <a href="http://gilmore-lab.github.io/temple-2017-02-27">HTML slides</a>. <a href="https://github.com/gilmore-lab/temple-2017-02-27/blob/master/index.md">Markdown</a>.</p>
<p>Gilmore, R.O., Fared, D.A., Dexheimer, M.G., &amp; Seisler, A.R. (2016, November). The appearance and disappearance of visual forms defined by differential motion evokes distinctive EEG responses in school-age children. Presentation at the Society for Neuroscience meeting in San Diego, CA. <a href="https://github.com/gilmore-lab/child-motion-form/blob/master/pubs/sfn-16-poster/poster_landscape.pdf">PDF</a>.</p>
<p>Gilmore, R.O. (2016, October). Go with the flow: The development of behavioral sensitivity and brain responses to optic flow. Talk at the Penn State Action club meeting. <a href="http://rawgit.com/gilmore-lab/psu-action-club-2016-10-28/master/gilmore-action-club.html">HTML slides</a>.</p>
<p>Jayaraman, S., Gilmore, R.O., &amp; Raudies, F. (2016, May). Changes in early optic flow experiences across development and culture. Talk at the International Congress on Infant Studies (ICIS) in New Orleans, LA. <a href="https://rawgit.com/gilmore-lab/ICIS-2016-New-Orleans/master/jayaraman-gilmore-raudies-ICIS-2016.html">HTML slides</a>.</p>
<p>Gilmore, R.O. (2016, September). Open science practices have made my work better. Talk at the Penn State Psychology Cognitive Area brown bag. <a href="https://cdn.rawgit.com/psu-psychology/cognitive/master/brown-bag/2015-09-09-gilmore/cog-bbag-2015-09-09.html">HTML slides</a>.</p>
<p>Adamiak, W., Thomas, A.L., Patel, S.M., &amp; Gilmore. R.O. (2015, May). Adult observers’ sensitivity to optic flow varies by pattern and speed. Poster presented at the Vision Sciences Society meeting, St.&nbsp;Pete’s Beach, FL. doi:10.1167/15.12.1008. <a href="../pdfs/adamiak-etal-vss-2015.pdf">PDF</a>. <a href="http://databrary.org/volume/73">Materials on Databrary</a>.</p>
<p>Raudies, F. &amp; Gilmore, R.O. (2014, May). An analysis of optic flow experienced by infants during natural activities. Poster presented at the Vision Sciences Society meeting, St.&nbsp;Pete Beach, FL. Journal of Vision, 14(10). 226. doi:10.1167/14.10.226. <a href="../pdf/raudies-etal-vss-2014.pdf">PDF</a></p>
<p>Thomas, A.L., Fesi, J.D. &amp; Gilmore, R.O. (2014, May). Temporal and speed tuning in brain responses to local and global motion patterns. Poster presented at the Vision Sciences Society meeting, St.&nbsp;Pete Beach, FL. Journal of Vision, 14(10). 482. doi:10.1167/14.10.482.</p>
<p>Fesi, J.D., Thomas, A.L., &amp; Gilmore, R.O. (2012, October). Distinct space-time sampling thresholds of VEP responses to optic flow. Poster presented at the Society for Neuroscience meeting, New Orleans, LA. <a href="../pdf/fesi-etal-sfn-2012.pdf">PDF</a></p>
<p>Gilmore, R.O., Raudies, F., Kretch, K.S., Franchak, J.M., &amp; Adolph, K.E. (2012, June). Do you see what I see? Comparing optic flow experienced by infants and their mothers. Poster presented at the International Conference on Infant Studies, Minneapolis, MN. <a href="../pdf/gilmore-etal-icis-2012.pdf">PDF</a>.</p>
<p>Fesi, J.D., Stiffler, J.R., &amp; Gilmore, R.O. (2012, May). Speed tuning of cortical responses to 2D figures defined by motion contrast is non-uniform across contrast types. Poster presented at the Vision Sciences Society meeting, Naples, FL.</p>
<p>Thomas, A.L., Mancino, A.C., Elnathan, H.C., Fesi, J.D., Hwang, K.R., &amp; Gilmore, R.O. (2012, May). Children’s cortical responses to optic flow patterns show differential tuning by pattern type, speed, scalp location, and age group. Poster presented at the Vision Sciences Society meeting, Naples, FL. <a href="../pdf/thomas-etal-vss-2012.pdf">PDF</a>.</p>
<p>Gilmore, R.O., Raudies, F., Kretch, K.S., Franchak, J.M., &amp; Adolph, K.E. (2012, May). Patterns of optic flow experienced by infants and their mothers during locomotion. Poster presented at the Vision Sciences Society meeting, Naples, FL. <a href="../pdf/gilmore-etal-vss-2012.pdf">PDF</a>.</p>
<p>Raudies, F., Kretch, K.S., Franchak, J.M., Mingolla, E., Gilmore, R.O., &amp; Adolph, K.E. (2012, May). Where do mothers point their head when they walk and where do babies point their head when they are carried? Poster presented at the Vision Sciences Society meeting, Naples, FL. <a href="../pdf/raudies-etal-vss-2012.pdf">PDF</a>.</p>
<p><strong>Materials</strong></p>
<ul>
<li><a href="http://databrary.org/volume/73" class="uri">http://databrary.org/volume/73</a></li>
<li><a href="http://dx.doi.org/10.17910/B7988V" class="uri">http://dx.doi.org/10.17910/B7988V</a></li>
</ul>
<p><strong>Role</strong></p>
<ul>
<li>Principal Investigator</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Florian Raudies, Hewlett-Packard Research</li>
<li>Swapnaa Jayaraman, Indiana University</li>
<li>Amanda Thomas, Swarthmore College</li>
<li>Jeremy Fesi, U.S. Marine Research</li>
</ul>
<p><strong>Support</strong></p>
<p>This project was supported by the National Science Foundation under grant <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1147440">BCS-1147440</a>.</p>
</section>
<section id="eye-tracking-technologies-to-characterize-and-optimize-visual-attending-in-down-syndrome" class="level2">
<h2 class="anchored" data-anchor-id="eye-tracking-technologies-to-characterize-and-optimize-visual-attending-in-down-syndrome">Eye Tracking Technologies to Characterize and Optimize Visual Attending in Down Syndrome</h2>
<p>Down Syndrome (DS) is the most common known genetic origin of intellectual disability and has an estimated incidence of 1 in every 1000 live births. Such children face unique challenges as they enter into the school years, because the speech that was previously adequate for communication with familiar partners in supportive settings is often not sufficient for academic communication with unfamiliar partners. Indeed, 95% of parents surveyed reported that their children with DS had difficulty being understood by persons outside their immediate social circle (Kumin, 1994). This has significant implications for academic, social, and vocational success; children with limited language skills are at risk of falling behind nondisabled peers academically and experiencing social isolation. Secondary issues often arise when children experience frustration in communication, commonly in the form of challenging behaviors. All aspects of development are further compromised when these behaviors involve aggression toward others, have significant health implications when they are self-injurious, and exacerbate service costs when they necessitate extensive behavior management plans. Children with DS are in desperate need of communication interventions that provide them with the tools to succeed throughout the school years. One form of intervention is called aided augmentative and alternative communication (AAC). In typical clinical applications, aided AAC systems employ picture books, tablet-style computers that present the user with graphic symbols, and sometimes text or synthesized voice output. Because AAC relies on vision rather than sound/speech for access to the communication messages, it is critical to map out how children with DS examine and extract information from visual AAC displays. Otherwise there is the risk of implementing systems that are poorly matched to children’s skills and needs, a practice that in turn results in limited use or abandonment of the system. Few current AAC designs consider the fit between the system and the visual processing skills of users, and most are uninformed by empirical knowledge about human visual information processing. Moreover, little is known about visual processing in persons with significant communication limitations associated with DS. This research aims to improve the design of AAC displays through characterization of visual attention patterns to different AAC displays and their effects on functional use. Eye tracking - rarely used in DS - will reveal attention patterns/processes that typically go unrecorded in behavioral research. Our three-phase program will begin with eye tracking studies of visual attention under largely non-social laboratory conditions. In the next phase, we will introduce social interactions and record gaze path using mobile eye tracking technology. In the final phase, we will translate the knowledge gained in the laboratory studies to optimize functional communication in individuals with DS in performing tasks that represent typical daily life activities.</p>
<p><strong>Role</strong></p>
<ul>
<li>Co-Investigator</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Krista Wilkinson, Penn State, Principal Investigator</li>
</ul>
<p><strong>Materials</strong></p>
<p>Gilmore, R.O. (2017). Wilkinson-lab. Code repository on GitHub. Retrieved December 19, 2017 from <a href="https://github.com/gilmore-lab/wilkinson-lab" class="uri">https://github.com/gilmore-lab/wilkinson-lab</a>.</p>
<p>Gilmore, R.O. (2017). Eye-tracking-DS. Code repository on GitHub. Retrieved December 19, 2017 from <a href="https://github.com/gilmore-lab/eye-tracking-DS" class="uri">https://github.com/gilmore-lab/eye-tracking-DS</a>.</p>
<p><strong>Support</strong></p>
<p>The project is supported by NICHD under <a href="https://projectreporter.nih.gov/project_info_description.cfm?aid=9194421&amp;icde=0">R01HD083381-02</a>.</p>
<p><strong>Publications</strong></p>
<p>Wilkinson, K.M., Gilmore, R.O., &amp; Qian, Y. (submitted). Judicious arrangement of symbols on a simulated AAC display optimizes visual attention by individuals with and without Down syndrome. <em>Journal of Speech, Language, and Hearing Research</em>.</p>
</section>
<section id="the-proximal-emotional-environment-project-peep" class="level2">
<h2 class="anchored" data-anchor-id="the-proximal-emotional-environment-project-peep">The Proximal Emotional Environment Project (PEEP)</h2>
<p><img src="https://nyu.databrary.org/slot/15068/0/asset/63850/download?inline=true" width="250px/"></p>
<p>A 5-year-old overhears her parents arguing loudly in the next room. She may not understand why they are arguing, but she realizes something is wrong because she perceives anger in their voices. Exposure to interpersonal conflict is consistently associated with less skillful emotion regulation in children although the mechanisms remain to be explained. Because inter-personal conflict is a heterogeneous phenomenon, investigation of the specific features of conflict that contribute to developmental pathways to emotional dysfunction and symptoms requires a process-oriented approach. In this project, we focus on brain responses to angry prosody in natural speech. We are studying young children’s neural processing of angry prosody, spoken by mothers and strangers, as a first step toward a future longitudinal study investigating how the neurocognitive processing of angry prosody mediates relations between conflict exposure in children and the development of anxiety- and anger-related symptoms.</p>
<p><strong>Publications</strong></p>
<p>Liu, P., Cole, P.M., Gilmore, R.O., Pérez-Edgar, K.E., Vigeant, M.C., Moriarty, P., Scherf, K.S. (in press). Young children’s neural processing of their mother’s voice: An fMRI study. <em>Neuropsychologia</em>. <a href="http://doi.org/10.1016/j.neuropsychologia.2018.12.003">doi: 10.1016/j.neuropsychologia.2018.12.003</a>.</p>
<p>Moriarty, P. M., Vigeant, M. C., Liu, P., Gilmore, R., Wolf, R., &amp; Cole, P. M. (2016). Low frequency analysis of acoustical parameters of emotional speech for use with functional magnetic resonance imaging. <em>The Journal of the Acoustical Society of America</em>, <em>140</em>(4), 3237–3238. Acoustical Society of America. Retrieved from https://doi.org/10.1121/1.4970240</p>
<p>Maggi, M., Cole, P., Elbich, D., Gilmore, R.O., Pérez–Edgar, K., Scherf, K.S. (in prep). Hearing emotions: School-aged children’s neural processing of the human voice and affective prosody.</p>
<p><strong>Presentations</strong></p>
<p>Stoop, T.B., Moriarty, P.M., Wolf, R., Gilmore, R.O., Perez-Edgar, K., Scherf, S, Vigeant, M.C., &amp; Cole, P.M. (in press). I know that voice! Mothers’ voices influence children’s perceptions of emotional intensity. <em>Journal of Experimental Child Psychology</em>.</p>
<p>Liu, P., Cole, P.M., Gilmore, R.O., Pérez-Edgar, K.E., Vigeant, M.C., Moriarty, P., Scherf, K.S. (2019). Young children’s neural processing of their mother’s voice: An fMRI study. Neuropsychologia, 122, 11-19. doi: 10.1016/j.neuropsychologia.2018.12.003.</p>
<p>Moriarty, P., Vigeant, M., Liu, P. Gilmore, R.O, &amp; Cole, P.M. (submitted) Comparing theory, consensus, and perception to the acoustics of emotional speech.</p>
<p>Moriarty, P.M., Vigeant, M., Wolf, R., Gilmore, R., &amp; Cole, P. (2018). Creation and characterization of an emotional speech database. <em>The Journal of the Acoustical Society of America</em>, <em>143</em>(3), 1869–1869. ASA. Retrieved from https://asa.scitation.org/doi/abs/10.1121/1.5036133</p>
<p><strong>Role</strong></p>
<ul>
<li>Co-investigator</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Pamela Cole, Penn State (PI)</li>
<li>Koraly Perez-Edgar, Penn State</li>
<li>Suzy Scherf, Penn State</li>
<li>Michelle Vigeant, Penn State</li>
<li>Pan Liu</li>
<li>Mirella Maggi</li>
<li>Peter Moriarty, Penn State</li>
<li>Tawni O’Dell, Penn State</li>
</ul>
<p><strong>Materials</strong></p>
<ul>
<li><a href="https://github.com/gilmore-lab/peep-II">PEEP II repo on GitHub</a>.</li>
<li><a href="https://github.com/gilmore-lab/PEEP-I">PEEP I repo on GitHub</a>.</li>
<li><a href="https://nyu.databrary.org/volume/248">PEEP I stimuli on Databrary</a></li>
</ul>
<p><strong>Support</strong></p>
<p>This project has received support from the Penn State <a href="http://ssri.psu.edu">Social Sciences Research Institute</a> and the National Institute of Mental Health under <a href="http://projectreporter.nih.gov/project_info_description.cfm?aid=8891792&amp;icde=26075719&amp;ddparam=&amp;ddvalue=&amp;ddsub=&amp;cr=2&amp;csb=default&amp;cs=ASC">R21-MH-104547</a>.</p>
</section>
<section id="eye-tracking-technologies-to-characterize-and-optimize-visual-attending-in-down-syndrome-1" class="level2">
<h2 class="anchored" data-anchor-id="eye-tracking-technologies-to-characterize-and-optimize-visual-attending-in-down-syndrome-1">Eye Tracking Technologies to Characterize and Optimize Visual Attending in Down Syndrome</h2>
<p>Down Syndrome (DS) is the most common known genetic origin of intellectual disability and has an estimated incidence of 1 in every 1000 live births. Such children face unique challenges as they enter into the school years, because the speech that was previously adequate for communication with familiar partners in supportive settings is often not sufficient for academic communication with unfamiliar partners. Indeed, 95% of parents surveyed reported that their children with DS had difficulty being understood by persons outside their immediate social circle (Kumin, 1994). This has significant implications for academic, social, and vocational success; children with limited language skills are at risk of falling behind nondisabled peers academically and experiencing social isolation. Secondary issues often arise when children experience frustration in communication, commonly in the form of challenging behaviors. All aspects of development are further compromised when these behaviors involve aggression toward others, have significant health implications when they are self-injurious, and exacerbate service costs when they necessitate extensive behavior management plans. Children with DS are in desperate need of communication interventions that provide them with the tools to succeed throughout the school years. One form of intervention is called aided augmentative and alternative communication (AAC). In typical clinical applications, aided AAC systems employ picture books, tablet-style computers that present the user with graphic symbols, and sometimes text or synthesized voice output. Because AAC relies on vision rather than sound/speech for access to the communication messages, it is critical to map out how children with DS examine and extract information from visual AAC displays. Otherwise there is the risk of implementing systems that are poorly matched to children’s skills and needs, a practice that in turn results in limited use or abandonment of the system. Few current AAC designs consider the fit between the system and the visual processing skills of users, and most are uninformed by empirical knowledge about human visual information processing. Moreover, little is known about visual processing in persons with significant communication limitations associated with DS. This research aims to improve the design of AAC displays through characterization of visual attention patterns to different AAC displays and their effects on functional use. Eye tracking - rarely used in DS - will reveal attention patterns/processes that typically go unrecorded in behavioral research. Our three-phase program will begin with eye tracking studies of visual attention under largely non-social laboratory conditions. In the next phase, we will introduce social interactions and record gaze path using mobile eye tracking technology. In the final phase, we will translate the knowledge gained in the laboratory studies to optimize functional communication in individuals with DS in performing tasks that represent typical daily life activities.</p>
<p><strong>Role</strong></p>
<ul>
<li>Co-Investigator</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Krista Wilkinson, Penn State, Principal Investigator</li>
</ul>
<p><strong>Materials</strong></p>
<p>Gilmore, R.O. (2017). Wilkinson-lab. Code repository on GitHub. Retrieved December 19, 2017 from <a href="https://github.com/gilmore-lab/wilkinson-lab" class="uri">https://github.com/gilmore-lab/wilkinson-lab</a>.</p>
<p>Gilmore, R.O. (2017). Eye-tracking-DS. Code repository on GitHub. Retrieved December 19, 2017 from <a href="https://github.com/gilmore-lab/eye-tracking-DS" class="uri">https://github.com/gilmore-lab/eye-tracking-DS</a>.</p>
<p><strong>Support</strong></p>
<p>The project is supported by NICHD under <a href="https://projectreporter.nih.gov/project_info_description.cfm?aid=9194421&amp;icde=0">R01HD083381-02</a>.</p>
</section>
<section id="computational-symmetry" class="level2">
<h2 class="anchored" data-anchor-id="computational-symmetry">Computational Symmetry</h2>
<p><img src="images/symmetry-sample-1.png" class="img-fluid" alt="Symmetric image"> <img src="images/symmetry-sample-2.png" class="img-fluid" alt="Symmetric image"></p>
<p>The ability to sense regular or near-regular patterns serves critical biological needs and is equally important for computer vision and machine intelligence. Despite wide variation in the types of regularity present in natural images, research on human and computer processing of pattern regularity has focused primarily on detecting bilateral reflection symmetry, using largely atheoretical approaches. The goals of this interdisciplinary research are to i) use principles of group theory to develop a conceptual framework for understanding regularity perception and brain activation in humans, and ii) to design general computer-based symmetry detection algorithms that can operate at a level of practical usability.</p>
<p><strong>Publication</strong></p>
<p>Kohler, P. J., Vedak, S., &amp; Gilmore, R. O. (2022). Perceptual similarities among wallpaper group exemplars. <em>Symmetry</em>, <em>14</em>(5), 857. <a href="https://doi.org/10.3390/sym14050857" class="uri">https://doi.org/10.3390/sym14050857</a>.</p>
<p>GitHub repo: <a href="https://github.com/gilmore-lab/symmetry-sorting">https://github.com/gilmore-lab/symmetry-sorting</a>.</p>
<p><strong>Presentations</strong></p>
<p>Vedak, S.C., Gilmore, R.O., Kohler, P.J., Liu, Y., &amp; Norcia, A.M. (2015, May). The salience of low-order visual features in highly self-similar wallpaper groups. Poster presented at the Vision Sciences Society meeting, St.&nbsp;Pete Beach, FL. <a href="../pdfs/vedak-etal-vss-2015.pdf">PDF</a>. <a href="http://databrary.org/volume/77">Materials on Databrary</a>.</p>
<p>Thomas, A.L., Gilmore, R.O., Norcia, A.M., Liu, Y., Fesi, J.D., Hwang, K.D., Stitt, J., &amp; Liu, J. (2012, October). Visual patterns with rotational symmetry activate distinct cortical regions. Poster presented at the Society for Neuroscience meeting, New Orleans, LA.</p>
<p><strong>Role</strong></p>
<ul>
<li>Co-Investigator</li>
</ul>
<p><strong>Collaborators</strong></p>
<ul>
<li>Yanxi Liu, Penn State Computer Science &amp; Engineering</li>
<li>Anthony Norcia, Stanford University, Department of Psychology</li>
<li>Peter Kohler, York University</li>
</ul>
<p><strong>Support</strong></p>
<p>This project was supported by the National Science Foundation under grant <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1248076">IIS-1248076</a>.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Qian2022-yp" class="csl-entry" role="listitem">
Qian, Y., Berenbaum, S. A., &amp; Gilmore, R. O. (2022). Vision contributes to sex differences in spatial cognition and activity interests. <em>Scientific Reports</em>, <em>12</em>, 17623. <a href="https://doi.org/10.1038/s41598-022-22269-y">https://doi.org/10.1038/s41598-022-22269-y</a>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>